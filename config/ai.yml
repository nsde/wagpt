# Learn more about what these changes do at https://platform.openai.com/docs/api-reference/completions/create

# The token count of your prompt plus max_tokens cannot exceed the model's context length.
# Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
# Tokens are - simplified - (parts of) words. Learn more here. https://platform.openai.com/tokenizer
max_tokens: 4096

# What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
temperature: 1

# Number between -2.0 and 2.0.
# Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
presence_penalty: 0

# Number between -2.0 and 2.0.
# Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
frequency_penalty: 0
